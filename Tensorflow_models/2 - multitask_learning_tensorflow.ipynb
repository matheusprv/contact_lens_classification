{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSZ35LQDjf3z",
        "outputId": "382dde11-0d97-4897-a68a-fa3ebcf5e238"
      },
      "outputs": [],
      "source": [
        "#Downloading dataset spreadsheet\n",
        "!gdown \n",
        "!gdown "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEEAHakMjmjL",
        "outputId": "51e10310-4f2e-4c0b-fbc2-61d940122dfc"
      },
      "outputs": [],
      "source": [
        "#Downloading dataset\n",
        "!gdown \n",
        "!unzip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wujr5wdfjrYM",
        "outputId": "833c15dc-e833-40f4-f4cf-cf14446b9571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.eager.context._EagerDeviceContext at 0x78ab16a32bc0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "tf.device('/device:GPU:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eQBMiiBbjtDN"
      },
      "outputs": [],
      "source": [
        "def mapped_labels(labels, colors, gender, black_white):\n",
        "    labels_to_lens = {'Colored': 0, 'Normal': 1, 'Transparent': 2}\n",
        "    lens_colors_types = {\"Gray\": 0, \"Hazel\": 1, \"Green\": 2, \"Blue\": 3}\n",
        "    gender_types = {\"Male\": 0, \"Female\": 1}\n",
        "    black_white_types = {\"No\": 0, \"Yes\": 1}\n",
        "\n",
        "    labels = float(labels_to_lens[labels])\n",
        "    gender = float(gender_types[gender])\n",
        "    black_white = float(black_white_types[black_white])\n",
        "    colors = float(lens_colors_types[colors])\n",
        "\n",
        "    return labels, gender, black_white, colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2XQep9wQjuSh"
      },
      "outputs": [],
      "source": [
        "def read_csv(train_test, number_of_itens = -1):\n",
        "    data_labels = []\n",
        "    imgs = []\n",
        "\n",
        "    csvreader = None\n",
        "    with open(f\"/content/CogentAnnotation{train_test}.csv\", \"r\") as file:\n",
        "        csvreader = csv.reader(file)\n",
        "        csvreader = list(csvreader)\n",
        "\n",
        "    csvreader.pop(0)\n",
        "    csvreader = np.array(csvreader)\n",
        "    np.random.seed(5)\n",
        "    np.random.shuffle(csvreader)\n",
        "\n",
        "    for row in csvreader:\n",
        "        labels, gender, black_white, colors = mapped_labels(row[1], row[3], row[4], row[5])\n",
        "\n",
        "        img_path = \"/content/\" + row[0].replace(\"&\", \"_\") + \".bmp\"\n",
        "        img = img_to_array(load_img(img_path, target_size=(300,300))) / 255.\n",
        "        imgs.append(img)\n",
        "\n",
        "        #data_labels.append([labels, colors, black_white, gender, row[6], row[7], row[8], row[9], row[10],row[11]])\n",
        "        data_labels.append([labels, colors, black_white, gender])\n",
        "\n",
        "\n",
        "    data_labels = (np.array(data_labels)).astype(np.float32)\n",
        "    return data_labels.tolist(), imgs\n",
        "\n",
        "treino_labels, treino_imgs = read_csv(\"Train\")\n",
        "test_labels, test_imgs = read_csv(\"Test\")\n",
        "\n",
        "#Converting some part of the test into training\n",
        "percentage_conversion = 0.7\n",
        "for i in range (int(len(test_labels) * percentage_conversion)):\n",
        "  treino_labels.append(test_labels.pop())\n",
        "  treino_imgs.append(test_imgs.pop())\n",
        "\n",
        "treino_labels = np.array(treino_labels).astype(np.float32).T\n",
        "treino_imgs = np.array(treino_imgs).astype(np.float32)\n",
        "del test_labels\n",
        "del test_imgs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GnA_Qxas-e1B"
      },
      "outputs": [],
      "source": [
        "from keras.backend import sigmoid\n",
        "from keras.layers import Activation\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "class Swish(Activation):\n",
        "\n",
        "    def _init_(self, activation, **kwargs):\n",
        "        super(Swish, self)._init_(activation, **kwargs)\n",
        "        self._name_ = 'swish'\n",
        "\n",
        "\n",
        "def swish(x, beta=1):\n",
        "    return x * sigmoid(beta * x)\n",
        "\n",
        "\n",
        "get_custom_objects().update({'swish': Swish(swish)})\n",
        "tf.keras.utils.get_custom_objects().update({'swish': Swish(swish)})\n",
        "get_custom_objects().update({'Swish': Swish(swish)})\n",
        "tf.keras.utils.get_custom_objects().update({'Swish': Swish(swish)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4sdKqfHjwkp",
        "outputId": "73d596dd-1974-4a28-9ff0-8a2da499a7b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
            "43941136/43941136 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Activation, Flatten, BatchNormalization, Rescaling, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
        "\n",
        "#Modelo base\n",
        "input = Input(shape=(300,300,3))\n",
        "rescaling = Rescaling(scale=1./255)(input) #1\n",
        "base_model = EfficientNetB3(input_shape=(300,300,3), include_top=False)\n",
        "base_model.trainable = True\n",
        "model = base_model(rescaling, training=True) #2\n",
        "batch = BatchNormalization()(model) #3\n",
        "globalPooling = GlobalAveragePooling2D()(batch) #4\n",
        "#Camada densa 2\n",
        "dropout1 = Dropout(0.2)(globalPooling)\n",
        "dense1 = Dense(512, activation='relu')(dropout1)\n",
        "#Camada densa 3\n",
        "dropout2 = Dropout(0.2)(dense1)\n",
        "dense2 = Dense(256, activation='relu')(dropout2)\n",
        "#Camada densa 3\n",
        "dropout3 = Dropout(0.2)(dense2)\n",
        "final_dense_shared = Dense(128, activation='relu')(dropout3)\n",
        "\n",
        "# Labels\n",
        "labels_prediction = Dense(3, activation='softmax', name='labels_prediction')(final_dense_shared)\n",
        "\n",
        "# Colors\n",
        "colors_prediction = Dense(4, activation='softmax', name='colors_prediction')(final_dense_shared)\n",
        "\n",
        "# Black and white\n",
        "black_and_white_prediction = Dense(1, activation='sigmoid', name='black_and_white_prediction')(final_dense_shared)\n",
        "\n",
        "# gender\n",
        "gender_prediction = Dense(1, activation='sigmoid', name='gender_prediction')(final_dense_shared)\n",
        "\n",
        "new_model = Model(inputs=input, outputs=[labels_prediction, colors_prediction, black_and_white_prediction, gender_prediction])\n",
        "\n",
        "losses = {\n",
        "    'labels_prediction': SparseCategoricalCrossentropy(),\n",
        "    'colors_prediction': SparseCategoricalCrossentropy(),\n",
        "    'black_and_white_prediction': BinaryCrossentropy(),\n",
        "    'gender_prediction': BinaryCrossentropy()\n",
        "}\n",
        "\n",
        "new_model.compile(optimizer=Adam(learning_rate=0.001), loss = losses, metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDczl3tfj3gB",
        "outputId": "9c6d13b6-a31f-445f-fa3d-33c9b563bdb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "149/149 [==============================] - 168s 545ms/step - loss: 2.9875 - labels_prediction_loss: 0.7636 - colors_prediction_loss: 1.3887 - black_and_white_prediction_loss: 0.2141 - gender_prediction_loss: 0.6212 - labels_prediction_accuracy: 0.5927 - colors_prediction_accuracy: 0.3083 - black_and_white_prediction_accuracy: 0.9492 - gender_prediction_accuracy: 0.6825 - val_loss: 2.7496 - val_labels_prediction_loss: 0.5985 - val_colors_prediction_loss: 1.3872 - val_black_and_white_prediction_loss: 0.1950 - val_gender_prediction_loss: 0.5689 - val_labels_prediction_accuracy: 0.6600 - val_colors_prediction_accuracy: 0.3132 - val_black_and_white_prediction_accuracy: 0.9497 - val_gender_prediction_accuracy: 0.7069\n",
            "Epoch 2/20\n",
            "149/149 [==============================] - 78s 526ms/step - loss: 2.6253 - labels_prediction_loss: 0.6334 - colors_prediction_loss: 1.3114 - black_and_white_prediction_loss: 0.1733 - gender_prediction_loss: 0.5072 - labels_prediction_accuracy: 0.6518 - colors_prediction_accuracy: 0.3721 - black_and_white_prediction_accuracy: 0.9526 - gender_prediction_accuracy: 0.7706 - val_loss: 2.6311 - val_labels_prediction_loss: 0.5262 - val_colors_prediction_loss: 1.3369 - val_black_and_white_prediction_loss: 0.1853 - val_gender_prediction_loss: 0.5828 - val_labels_prediction_accuracy: 0.7219 - val_colors_prediction_accuracy: 0.3585 - val_black_and_white_prediction_accuracy: 0.9497 - val_gender_prediction_accuracy: 0.7270\n",
            "Epoch 3/20\n",
            "149/149 [==============================] - 79s 531ms/step - loss: 2.3999 - labels_prediction_loss: 0.5884 - colors_prediction_loss: 1.2057 - black_and_white_prediction_loss: 0.1669 - gender_prediction_loss: 0.4388 - labels_prediction_accuracy: 0.6669 - colors_prediction_accuracy: 0.4526 - black_and_white_prediction_accuracy: 0.9526 - gender_prediction_accuracy: 0.8024 - val_loss: 2.7561 - val_labels_prediction_loss: 0.5491 - val_colors_prediction_loss: 1.4158 - val_black_and_white_prediction_loss: 0.1853 - val_gender_prediction_loss: 0.6058 - val_labels_prediction_accuracy: 0.6633 - val_colors_prediction_accuracy: 0.3400 - val_black_and_white_prediction_accuracy: 0.9497 - val_gender_prediction_accuracy: 0.7337\n",
            "Epoch 4/20\n",
            "149/149 [==============================] - 80s 537ms/step - loss: 2.1218 - labels_prediction_loss: 0.5607 - colors_prediction_loss: 1.0631 - black_and_white_prediction_loss: 0.1229 - gender_prediction_loss: 0.3751 - labels_prediction_accuracy: 0.6867 - colors_prediction_accuracy: 0.5566 - black_and_white_prediction_accuracy: 0.9564 - gender_prediction_accuracy: 0.8385 - val_loss: 2.8187 - val_labels_prediction_loss: 0.5279 - val_colors_prediction_loss: 1.5240 - val_black_and_white_prediction_loss: 0.1700 - val_gender_prediction_loss: 0.5967 - val_labels_prediction_accuracy: 0.6968 - val_colors_prediction_accuracy: 0.3618 - val_black_and_white_prediction_accuracy: 0.9497 - val_gender_prediction_accuracy: 0.7437\n",
            "Epoch 5/20\n",
            "149/149 [==============================] - 75s 507ms/step - loss: 1.8565 - labels_prediction_loss: 0.5565 - colors_prediction_loss: 0.8923 - black_and_white_prediction_loss: 0.1040 - gender_prediction_loss: 0.3036 - labels_prediction_accuracy: 0.6951 - colors_prediction_accuracy: 0.6510 - black_and_white_prediction_accuracy: 0.9677 - gender_prediction_accuracy: 0.8700 - val_loss: 2.7744 - val_labels_prediction_loss: 0.6941 - val_colors_prediction_loss: 1.4254 - val_black_and_white_prediction_loss: 0.1425 - val_gender_prediction_loss: 0.5124 - val_labels_prediction_accuracy: 0.6549 - val_colors_prediction_accuracy: 0.4070 - val_black_and_white_prediction_accuracy: 0.9581 - val_gender_prediction_accuracy: 0.7722\n",
            "Epoch 6/20\n",
            "149/149 [==============================] - 76s 508ms/step - loss: 1.6585 - labels_prediction_loss: 0.5262 - colors_prediction_loss: 0.7833 - black_and_white_prediction_loss: 0.0880 - gender_prediction_loss: 0.2611 - labels_prediction_accuracy: 0.7219 - colors_prediction_accuracy: 0.7068 - black_and_white_prediction_accuracy: 0.9677 - gender_prediction_accuracy: 0.8914 - val_loss: 2.4696 - val_labels_prediction_loss: 0.5502 - val_colors_prediction_loss: 1.2813 - val_black_and_white_prediction_loss: 0.1213 - val_gender_prediction_loss: 0.5168 - val_labels_prediction_accuracy: 0.6935 - val_colors_prediction_accuracy: 0.5209 - val_black_and_white_prediction_accuracy: 0.9598 - val_gender_prediction_accuracy: 0.7990\n",
            "Epoch 7/20\n",
            "149/149 [==============================] - 80s 539ms/step - loss: 1.3296 - labels_prediction_loss: 0.4877 - colors_prediction_loss: 0.5996 - black_and_white_prediction_loss: 0.0627 - gender_prediction_loss: 0.1796 - labels_prediction_accuracy: 0.7580 - colors_prediction_accuracy: 0.7706 - black_and_white_prediction_accuracy: 0.9757 - gender_prediction_accuracy: 0.9299 - val_loss: 2.7155 - val_labels_prediction_loss: 0.4925 - val_colors_prediction_loss: 1.5452 - val_black_and_white_prediction_loss: 0.1552 - val_gender_prediction_loss: 0.5227 - val_labels_prediction_accuracy: 0.7605 - val_colors_prediction_accuracy: 0.5126 - val_black_and_white_prediction_accuracy: 0.9581 - val_gender_prediction_accuracy: 0.8057\n",
            "Epoch 8/20\n",
            "149/149 [==============================] - 80s 539ms/step - loss: 1.0881 - labels_prediction_loss: 0.4061 - colors_prediction_loss: 0.4654 - black_and_white_prediction_loss: 0.0655 - gender_prediction_loss: 0.1511 - labels_prediction_accuracy: 0.8117 - colors_prediction_accuracy: 0.8268 - black_and_white_prediction_accuracy: 0.9753 - gender_prediction_accuracy: 0.9409 - val_loss: 2.2356 - val_labels_prediction_loss: 0.4887 - val_colors_prediction_loss: 1.3035 - val_black_and_white_prediction_loss: 0.1264 - val_gender_prediction_loss: 0.3170 - val_labels_prediction_accuracy: 0.7621 - val_colors_prediction_accuracy: 0.5595 - val_black_and_white_prediction_accuracy: 0.9598 - val_gender_prediction_accuracy: 0.8626\n",
            "Epoch 9/20\n",
            "149/149 [==============================] - 80s 540ms/step - loss: 0.9261 - labels_prediction_loss: 0.3290 - colors_prediction_loss: 0.4301 - black_and_white_prediction_loss: 0.0469 - gender_prediction_loss: 0.1201 - labels_prediction_accuracy: 0.8549 - colors_prediction_accuracy: 0.8473 - black_and_white_prediction_accuracy: 0.9836 - gender_prediction_accuracy: 0.9568 - val_loss: 1.8642 - val_labels_prediction_loss: 0.3962 - val_colors_prediction_loss: 0.8452 - val_black_and_white_prediction_loss: 0.1461 - val_gender_prediction_loss: 0.4767 - val_labels_prediction_accuracy: 0.8241 - val_colors_prediction_accuracy: 0.6834 - val_black_and_white_prediction_accuracy: 0.9531 - val_gender_prediction_accuracy: 0.8275\n",
            "Epoch 10/20\n",
            "149/149 [==============================] - 80s 539ms/step - loss: 0.8420 - labels_prediction_loss: 0.3217 - colors_prediction_loss: 0.3471 - black_and_white_prediction_loss: 0.0452 - gender_prediction_loss: 0.1279 - labels_prediction_accuracy: 0.8788 - colors_prediction_accuracy: 0.8763 - black_and_white_prediction_accuracy: 0.9849 - gender_prediction_accuracy: 0.9480 - val_loss: 1.6072 - val_labels_prediction_loss: 0.3656 - val_colors_prediction_loss: 0.9129 - val_black_and_white_prediction_loss: 0.0932 - val_gender_prediction_loss: 0.2355 - val_labels_prediction_accuracy: 0.8442 - val_colors_prediction_accuracy: 0.6700 - val_black_and_white_prediction_accuracy: 0.9598 - val_gender_prediction_accuracy: 0.9079\n",
            "Epoch 11/20\n",
            "149/149 [==============================] - 80s 536ms/step - loss: 0.6812 - labels_prediction_loss: 0.2498 - colors_prediction_loss: 0.2900 - black_and_white_prediction_loss: 0.0332 - gender_prediction_loss: 0.1082 - labels_prediction_accuracy: 0.8964 - colors_prediction_accuracy: 0.9039 - black_and_white_prediction_accuracy: 0.9866 - gender_prediction_accuracy: 0.9555 - val_loss: 1.5227 - val_labels_prediction_loss: 0.3230 - val_colors_prediction_loss: 0.8481 - val_black_and_white_prediction_loss: 0.1227 - val_gender_prediction_loss: 0.2288 - val_labels_prediction_accuracy: 0.8693 - val_colors_prediction_accuracy: 0.7320 - val_black_and_white_prediction_accuracy: 0.9732 - val_gender_prediction_accuracy: 0.9246\n",
            "Epoch 12/20\n",
            "149/149 [==============================] - 80s 536ms/step - loss: 0.6038 - labels_prediction_loss: 0.2276 - colors_prediction_loss: 0.2621 - black_and_white_prediction_loss: 0.0324 - gender_prediction_loss: 0.0817 - labels_prediction_accuracy: 0.9052 - colors_prediction_accuracy: 0.9115 - black_and_white_prediction_accuracy: 0.9904 - gender_prediction_accuracy: 0.9702 - val_loss: 1.8366 - val_labels_prediction_loss: 0.3392 - val_colors_prediction_loss: 0.8350 - val_black_and_white_prediction_loss: 0.1086 - val_gender_prediction_loss: 0.5538 - val_labels_prediction_accuracy: 0.8559 - val_colors_prediction_accuracy: 0.7169 - val_black_and_white_prediction_accuracy: 0.9598 - val_gender_prediction_accuracy: 0.8107\n",
            "Epoch 13/20\n",
            "149/149 [==============================] - 80s 538ms/step - loss: 0.5203 - labels_prediction_loss: 0.2063 - colors_prediction_loss: 0.1980 - black_and_white_prediction_loss: 0.0321 - gender_prediction_loss: 0.0838 - labels_prediction_accuracy: 0.9258 - colors_prediction_accuracy: 0.9346 - black_and_white_prediction_accuracy: 0.9916 - gender_prediction_accuracy: 0.9685 - val_loss: 1.5681 - val_labels_prediction_loss: 0.3532 - val_colors_prediction_loss: 0.8184 - val_black_and_white_prediction_loss: 0.0492 - val_gender_prediction_loss: 0.3473 - val_labels_prediction_accuracy: 0.8744 - val_colors_prediction_accuracy: 0.7588 - val_black_and_white_prediction_accuracy: 0.9816 - val_gender_prediction_accuracy: 0.8827\n",
            "Epoch 14/20\n",
            "149/149 [==============================] - 80s 540ms/step - loss: 0.4377 - labels_prediction_loss: 0.1744 - colors_prediction_loss: 0.1812 - black_and_white_prediction_loss: 0.0281 - gender_prediction_loss: 0.0541 - labels_prediction_accuracy: 0.9362 - colors_prediction_accuracy: 0.9442 - black_and_white_prediction_accuracy: 0.9929 - gender_prediction_accuracy: 0.9836 - val_loss: 1.3997 - val_labels_prediction_loss: 0.4255 - val_colors_prediction_loss: 0.6705 - val_black_and_white_prediction_loss: 0.1008 - val_gender_prediction_loss: 0.2029 - val_labels_prediction_accuracy: 0.8157 - val_colors_prediction_accuracy: 0.7839 - val_black_and_white_prediction_accuracy: 0.9598 - val_gender_prediction_accuracy: 0.9280\n",
            "Epoch 15/20\n",
            "149/149 [==============================] - 80s 539ms/step - loss: 0.4496 - labels_prediction_loss: 0.1526 - colors_prediction_loss: 0.1969 - black_and_white_prediction_loss: 0.0261 - gender_prediction_loss: 0.0740 - labels_prediction_accuracy: 0.9409 - colors_prediction_accuracy: 0.9312 - black_and_white_prediction_accuracy: 0.9904 - gender_prediction_accuracy: 0.9723 - val_loss: 1.5352 - val_labels_prediction_loss: 0.3558 - val_colors_prediction_loss: 0.8142 - val_black_and_white_prediction_loss: 0.0739 - val_gender_prediction_loss: 0.2913 - val_labels_prediction_accuracy: 0.8777 - val_colors_prediction_accuracy: 0.7420 - val_black_and_white_prediction_accuracy: 0.9732 - val_gender_prediction_accuracy: 0.8844\n",
            "Epoch 16/20\n",
            "149/149 [==============================] - 75s 504ms/step - loss: 0.3536 - labels_prediction_loss: 0.1418 - colors_prediction_loss: 0.1358 - black_and_white_prediction_loss: 0.0190 - gender_prediction_loss: 0.0570 - labels_prediction_accuracy: 0.9459 - colors_prediction_accuracy: 0.9543 - black_and_white_prediction_accuracy: 0.9933 - gender_prediction_accuracy: 0.9794 - val_loss: 1.4818 - val_labels_prediction_loss: 0.2985 - val_colors_prediction_loss: 0.8502 - val_black_and_white_prediction_loss: 0.0777 - val_gender_prediction_loss: 0.2553 - val_labels_prediction_accuracy: 0.9095 - val_colors_prediction_accuracy: 0.7772 - val_black_and_white_prediction_accuracy: 0.9765 - val_gender_prediction_accuracy: 0.9380\n",
            "Epoch 17/20\n",
            "149/149 [==============================] - 80s 536ms/step - loss: 0.3617 - labels_prediction_loss: 0.1236 - colors_prediction_loss: 0.1581 - black_and_white_prediction_loss: 0.0216 - gender_prediction_loss: 0.0583 - labels_prediction_accuracy: 0.9522 - colors_prediction_accuracy: 0.9438 - black_and_white_prediction_accuracy: 0.9916 - gender_prediction_accuracy: 0.9807 - val_loss: 1.4174 - val_labels_prediction_loss: 0.2837 - val_colors_prediction_loss: 0.6932 - val_black_and_white_prediction_loss: 0.1546 - val_gender_prediction_loss: 0.2859 - val_labels_prediction_accuracy: 0.9095 - val_colors_prediction_accuracy: 0.7923 - val_black_and_white_prediction_accuracy: 0.9698 - val_gender_prediction_accuracy: 0.9079\n",
            "Epoch 18/20\n",
            "149/149 [==============================] - 80s 535ms/step - loss: 0.3192 - labels_prediction_loss: 0.1441 - colors_prediction_loss: 0.1042 - black_and_white_prediction_loss: 0.0156 - gender_prediction_loss: 0.0552 - labels_prediction_accuracy: 0.9488 - colors_prediction_accuracy: 0.9643 - black_and_white_prediction_accuracy: 0.9958 - gender_prediction_accuracy: 0.9799 - val_loss: 1.2006 - val_labels_prediction_loss: 0.2750 - val_colors_prediction_loss: 0.6651 - val_black_and_white_prediction_loss: 0.0377 - val_gender_prediction_loss: 0.2227 - val_labels_prediction_accuracy: 0.8945 - val_colors_prediction_accuracy: 0.8074 - val_black_and_white_prediction_accuracy: 0.9866 - val_gender_prediction_accuracy: 0.9397\n",
            "Epoch 19/20\n",
            "149/149 [==============================] - 80s 534ms/step - loss: 0.3527 - labels_prediction_loss: 0.1243 - colors_prediction_loss: 0.1405 - black_and_white_prediction_loss: 0.0212 - gender_prediction_loss: 0.0667 - labels_prediction_accuracy: 0.9530 - colors_prediction_accuracy: 0.9534 - black_and_white_prediction_accuracy: 0.9933 - gender_prediction_accuracy: 0.9765 - val_loss: 1.5005 - val_labels_prediction_loss: 0.2510 - val_colors_prediction_loss: 0.8907 - val_black_and_white_prediction_loss: 0.1099 - val_gender_prediction_loss: 0.2488 - val_labels_prediction_accuracy: 0.9095 - val_colors_prediction_accuracy: 0.7219 - val_black_and_white_prediction_accuracy: 0.9682 - val_gender_prediction_accuracy: 0.9112\n",
            "Epoch 20/20\n",
            "149/149 [==============================] - 79s 533ms/step - loss: 0.3136 - labels_prediction_loss: 0.1280 - colors_prediction_loss: 0.1362 - black_and_white_prediction_loss: 0.0148 - gender_prediction_loss: 0.0347 - labels_prediction_accuracy: 0.9560 - colors_prediction_accuracy: 0.9593 - black_and_white_prediction_accuracy: 0.9954 - gender_prediction_accuracy: 0.9870 - val_loss: 1.5234 - val_labels_prediction_loss: 0.3191 - val_colors_prediction_loss: 0.8244 - val_black_and_white_prediction_loss: 0.1073 - val_gender_prediction_loss: 0.2726 - val_labels_prediction_accuracy: 0.8827 - val_colors_prediction_accuracy: 0.7655 - val_black_and_white_prediction_accuracy: 0.9715 - val_gender_prediction_accuracy: 0.9213\n",
            "<keras.callbacks.History object at 0x78aaec3e3d00>\n"
          ]
        }
      ],
      "source": [
        "history = new_model.fit(\n",
        "    treino_imgs,\n",
        "    [treino_labels[0], treino_labels[1], treino_labels[2], treino_labels[3]],\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2\n",
        ")\n",
        "print(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX-i1w6t4p57",
        "outputId": "eb898b0b-d6a3-4a6d-a650-4653103cf734"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  del treino_imgs\n",
        "  del treino_labels\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  del test_labels\n",
        "  del test_imgs\n",
        "except:\n",
        "  pass\n",
        "\n",
        "test_labels, test_imgs = read_csv(\"Test\")\n",
        "#Removing the itens that have gone to the training\n",
        "for i in range (int(len(test_labels) * percentage_conversion)):\n",
        "  test_labels.pop()\n",
        "  test_imgs.pop()\n",
        "\n",
        "test_labels = np.array(test_labels).astype(np.float32).T\n",
        "test_imgs = np.array(test_imgs).astype(np.float32)\n",
        "\n",
        "results = new_model.evaluate(test_imgs, [test_labels[0], test_labels[1], test_labels[2], test_labels[3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVE_48iB5Rsh",
        "outputId": "0d718ab4-5bc5-4235-d9bc-2bcb785e647d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qtd itens para teste:      527\n",
            "Labels:                89.18 %\n",
            "Colors:                77.04 %\n",
            "Black/White:           95.83 %\n",
            "Genero:                95.07 %\n"
          ]
        }
      ],
      "source": [
        "print(\"Qtd itens para teste:     \", test_labels.shape[1])\n",
        "print(\"Labels:               \", round(results[5]*100, 2), \"%\")\n",
        "print(\"Colors:               \", round(results[6]*100, 2), \"%\")\n",
        "print(\"Black/White:          \", round(results[7]*100, 2), \"%\")\n",
        "print(\"Genero:               \", round(results[8]*100, 2), \"%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
